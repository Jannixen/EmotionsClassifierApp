{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_methods.helper_methods import *\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import splitfolders as sf\n",
    "import pathlib\n",
    "from sklearn.metrics import *\n",
    "from statistics import mean\n",
    "from PIL import Image\n",
    "import os.path\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam,SGD,RMSprop\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.initializers import ones,RandomNormal,HeNormal\n",
    "from tensorflow.keras.regularizers import *\n",
    "from tensorflow import losses\n",
    "\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Flatten, LeakyReLU, Dense, Dropout\n",
    "import matplotlib.pyplot as plt \n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('always')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zbalansowanie \n",
    "Link do zbioru: [affectnetsample](https://www.kaggle.com/mouadriali/affectnetsample)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "face_categories = ['anger', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
    "n_categories = len(face_categories)\n",
    "face_categories_mapping = dict(zip(sorted(face_categories),range(8)))\n",
    "category_frequencies = pd.DataFrame({\n",
    "    \"face_category\" : face_categories, \n",
    "    \"frequency\" : [len(os.listdir(f\"dataset/{c}\")) for c in face_categories]\n",
    "}).sort_values(by=\"frequency\")\n",
    "sns.barplot(x=\"face_category\",y=\"frequency\",data=category_frequencies);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anger': 0,\n",
       " 'disgust': 1,\n",
       " 'fear': 2,\n",
       " 'happy': 3,\n",
       " 'neutral': 4,\n",
       " 'sad': 5,\n",
       " 'surprise': 6}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face_categories_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>face_category</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>disgust</td>\n",
       "      <td>547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>surprise</td>\n",
       "      <td>4002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anger</td>\n",
       "      <td>4953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fear</td>\n",
       "      <td>5121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sad</td>\n",
       "      <td>6077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>6198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>happy</td>\n",
       "      <td>8989</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  face_category  frequency\n",
       "1       disgust        547\n",
       "6      surprise       4002\n",
       "0         anger       4953\n",
       "2          fear       5121\n",
       "5           sad       6077\n",
       "4       neutral       6198\n",
       "3         happy       8989"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìê Rozmiary Obrazk√≥w\n",
    "\n",
    "Poniewa≈º sieƒá musi mieƒá okre≈õlonƒÖ szeroko≈õƒá wej≈õcia wsp√≥lnƒÖ dla wszystkich danych uczƒÖcych,testowych i klasyfikowanych, nale≈ºy poznaƒá rozmiary obrazk√≥w w zbiorze, aby dobraƒá odpowiedniƒÖ wielko≈õƒá wej≈õcia modelu. Mo≈ºe to trochƒô potrwaƒá (oko≈Ço 2 minuty)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_image_sizes = []\n",
    "# for subdirectory in [\"train\",\"val\",\"test\"]:\n",
    "#     print(subdirectory)\n",
    "#     for category in face_categories:\n",
    "#         print(category)\n",
    "#         base_path = f\"faces/{subdirectory}/{category}\"\n",
    "#         files = os.listdir(base_path)\n",
    "#         for file in files:\n",
    "#             path = os.path.join(base_path,file)\n",
    "#             img = Image.open(path)\n",
    "#             all_image_sizes.append(img.size)\n",
    "# # WyglƒÖda na to, ≈ºe najmniejszy obrazek ma rozmiar 224 x 224 piksele\n",
    "# sorted(all_image_sizes, key=lambda x:x[0])[0], sorted(all_image_sizes, key=lambda x:x[1])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_image_sizes = []\n",
    "# for category in face_categories:\n",
    "#     print(category)\n",
    "#     base_path = f\"dataset/{category}\"\n",
    "#     files = os.listdir(base_path)\n",
    "#     for file in files:\n",
    "#         path = os.path.join(base_path,file)\n",
    "#         img = Image.open(path)\n",
    "#         all_image_sizes.append(img.size)\n",
    "# sorted(all_image_sizes, key=lambda x:x[0])[0], sorted(all_image_sizes, key=lambda x:x[1])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Podzia≈Ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sf.ratio(\"dataset\", output=\"dataset_splitted\", seed=1, ratio=(0.8,0.2), group_prefix=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wagi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'dataset_splitted/train/anger'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\matlaczj\\Desktop\\pz1_kosz\\emotion_classifier.ipynb Cell 14'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/matlaczj/Desktop/pz1_kosz/emotion_classifier.ipynb#ch0000013?line=0'>1</a>\u001b[0m category_frequencies \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39mzip\u001b[39m(face_categories,[\u001b[39mlen\u001b[39m(os\u001b[39m.\u001b[39mlistdir(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdataset_splitted/train/\u001b[39m\u001b[39m{\u001b[39;00mc\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)) \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m face_categories]))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/matlaczj/Desktop/pz1_kosz/emotion_classifier.ipynb#ch0000013?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmath\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/matlaczj/Desktop/pz1_kosz/emotion_classifier.ipynb#ch0000013?line=3'>4</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate_class_weight\u001b[39m(labels_dict,mu\u001b[39m=\u001b[39m\u001b[39m0.15\u001b[39m):\n",
      "\u001b[1;32mc:\\Users\\matlaczj\\Desktop\\pz1_kosz\\emotion_classifier.ipynb Cell 14'\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/matlaczj/Desktop/pz1_kosz/emotion_classifier.ipynb#ch0000013?line=0'>1</a>\u001b[0m category_frequencies \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39mzip\u001b[39m(face_categories,[\u001b[39mlen\u001b[39m(os\u001b[39m.\u001b[39;49mlistdir(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mdataset_splitted/train/\u001b[39;49m\u001b[39m{\u001b[39;49;00mc\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)) \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m face_categories]))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/matlaczj/Desktop/pz1_kosz/emotion_classifier.ipynb#ch0000013?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmath\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/matlaczj/Desktop/pz1_kosz/emotion_classifier.ipynb#ch0000013?line=3'>4</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate_class_weight\u001b[39m(labels_dict,mu\u001b[39m=\u001b[39m\u001b[39m0.15\u001b[39m):\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'dataset_splitted/train/anger'"
     ]
    }
   ],
   "source": [
    "category_frequencies = dict(zip(face_categories,[len(os.listdir(f\"dataset_splitted/train/{c}\")) for c in face_categories]))\n",
    "\n",
    "import math\n",
    "def create_class_weight(labels_dict,mu=0.15):\n",
    "    total = np.sum(list(labels_dict.values()))\n",
    "    keys = labels_dict.keys()\n",
    "    class_weight = dict()\n",
    "    for idx,key in enumerate(keys):\n",
    "        score = math.log(mu*total/float(labels_dict[key]))\n",
    "        class_weight[idx] = score if score > 1.0 else 1.0\n",
    "    return class_weight\n",
    "\n",
    "weights = create_class_weight(category_frequencies,mu=1)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìô Wczytanie i wstƒôpne przetworzenie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"C:\\\\Users\\\\matlaczj\\\\Documents\\\\Projekt_PZ1\\\\dataset_splitted\\\\\"\n",
    "train_dir,val_dir,test_dir,train_aug_dir,pred_dir = [pathlib.Path(f\"{base_dir}{t}\") for t in [\"train\",\"val\",\"test\",\"train_aug\",\"pred\"]]\n",
    "train_batch_size = 512\n",
    "val_test_batch_size = 512\n",
    "img_height = 48\n",
    "img_width = 48"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generowanie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # wszystkie transformacje wykonujemy tylko na zbiorze treningowym\n",
    "# # dogenerujƒô dane w celu zmniejszenia wariancji modelu\n",
    "# train_datagen = ImageDataGenerator(\n",
    "#         #rescale=1./255,\n",
    "#         rotation_range = 30, \n",
    "#         width_shift_range = 0.15, \n",
    "#         height_shift_range = 0.15, \n",
    "#         shear_range = 0.2, \n",
    "#         zoom_range = 0.2, \n",
    "#         brightness_range = (0.2,0.8), \n",
    "#         horizontal_flip = True\n",
    "#     )\n",
    "\n",
    "# for i, one_category in enumerate(os.listdir('dataset_splitted/train')):\n",
    "#     os.mkdir(f\"dataset_splitted/train_aug/{one_category}\")\n",
    "#     train_generator = train_datagen.flow_from_directory(\n",
    "#             train_dir,\n",
    "#             #color_mode = \"grayscale\",\n",
    "#             target_size=(img_height, img_width),\n",
    "#             batch_size=train_batch_size,\n",
    "#             classes = [one_category],\n",
    "#             save_to_dir = os.path.join(\"dataset_splitted\",\"train_aug\",one_category),\n",
    "#             class_mode='sparse',\n",
    "#             shuffle=True)\n",
    "\n",
    "#     print(one_category)\n",
    "#     maxindx = len(train_generator)\n",
    "#     for idx,i in enumerate(range(len(train_generator))):\n",
    "#         print(f\"{idx}/{maxindx}\",end=\" \")\n",
    "#         train_generator.next()\n",
    "#     print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = image_dataset_from_directory(\n",
    "    directory=train_dir,\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=train_batch_size,\n",
    "    image_size=(img_height, img_width),\n",
    "    shuffle=True,\n",
    "    seed=1,\n",
    "    interpolation='bilinear',\n",
    "    smart_resize = True\n",
    ")\n",
    "\n",
    "# augmented_training_ds = image_dataset_from_directory(\n",
    "#     directory=train_aug_dir,\n",
    "#     labels='inferred',\n",
    "#     label_mode='int',\n",
    "#     #color_mode=\"grayscale\",\n",
    "#     batch_size=train_batch_size,\n",
    "#     image_size=(img_height, img_width),\n",
    "#     shuffle=True,\n",
    "#     seed=1,\n",
    "#     interpolation='bilinear',\n",
    "#     smart_resize = True\n",
    "# )\n",
    "\n",
    "# train_ds = train_ds.concatenate(augmented_training_ds)\n",
    "train_ds = train_ds.shuffle(seed=1,buffer_size=1000,reshuffle_each_iteration=True)\n",
    "\n",
    "val_ds = image_dataset_from_directory(\n",
    "    directory=val_dir,\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=val_test_batch_size,\n",
    "    image_size=(img_height, img_width),\n",
    "    shuffle=True,\n",
    "    seed=1,\n",
    "    interpolation='bilinear',\n",
    "    smart_resize = True\n",
    ")\n",
    "\n",
    "# test_ds = image_dataset_from_directory(\n",
    "#     directory=test_dir,\n",
    "#     labels='inferred',\n",
    "#     label_mode='int',\n",
    "#     color_mode=\"grayscale\",\n",
    "#     batch_size=val_test_batch_size,\n",
    "#     image_size=(img_height, img_width),\n",
    "#     shuffle=True,\n",
    "#     seed=1,\n",
    "#     interpolation='bilinear',\n",
    "#     smart_resize = True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üì∫ OglƒÖdanie Obrazk√≥w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows = 3\n",
    "# cols = 3\n",
    "# if(rows*cols > train_batch_size):\n",
    "#     print(\"Too many pictures for current batch_size\")\n",
    "# else:\n",
    "#     plt.figure(figsize=(15, 15))\n",
    "#     for images, labels in train_ds.take(1):\n",
    "#         for i in range(rows*cols):\n",
    "#             ax = plt.subplot(rows, cols, i + 1)\n",
    "#             plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "#             plt.title(face_categories[labels[i]])\n",
    "#             plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üò∏ \"Buforowanie Wstƒôpne\"\n",
    "zbior√≥w train/val, by kieszeniowaƒá na gpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî• Topologia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), input_shape=(48,48,1),padding=\"same\"),\n",
    "    LeakyReLU(),\n",
    "    Conv2D(32, (3,3), padding=\"same\"),\n",
    "    LeakyReLU(),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    Conv2D(64,(3,3),padding=\"same\"),\n",
    "    LeakyReLU(),\n",
    "    Conv2D(64,(3,3), padding=\"same\"),\n",
    "    LeakyReLU(),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    Conv2D(128,(3,3),padding=\"same\"),\n",
    "    LeakyReLU(),\n",
    "    Conv2D(128,(3,3), padding=\"same\"),\n",
    "    LeakyReLU(),\n",
    "    Conv2D(128,(3,3), padding=\"same\"),\n",
    "    LeakyReLU(),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Dropout(0.4),\n",
    "    \n",
    "    Flatten(),\n",
    "    \n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.4),\n",
    "    \n",
    "    Dense(n_categories, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Podsumowanie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kompilacja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-4),\n",
    "    loss=losses.SparseCategoricalCrossentropy(),#from_logits=True\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateReducerCb(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self,epoch,logs={}):\n",
    "        old_lr = self.model.optimizer.lr.read_value()\n",
    "        new_lr = 0.99 * old_lr\n",
    "        self.model.optimizer.lr.assign(new_lr)\n",
    "        old_lr,new_lr = round(float(old_lr),6),round(float(new_lr),6)\n",
    "        print(f\"Reducing LR {old_lr}->{new_lr}\")\n",
    "\n",
    "class ExtendedHistoryCb(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self,logs={}):\n",
    "        # Zmienne na ca≈Çy trening u≈ºywane p√≥≈∫niej do narysowania wykres√≥w.\n",
    "        # Wyniki r,p dla ka≈ºdej klasy\n",
    "        self.val_recalls = []\n",
    "        self.val_precisions = []\n",
    "        self.val_f1s = []\n",
    "\n",
    "        self.avg_val_recall = []\n",
    "        self.avg_val_precision = []\n",
    "        self.avg_val_f1 = []\n",
    "        # Specjalnie dla early stoppingu.\n",
    "        self.avg_val_f1_history = []\n",
    "\n",
    "    def on_epoch_end(self,batch,logs={}):\n",
    "        val_predictions = tf.math.argmax(model.predict(val_ds),axis=1).numpy()\n",
    "        val_labels = np.concatenate([y for x,y in val_ds],axis=0)\n",
    "\n",
    "        # ZwracajƒÖ wyniki dla ka≈ºdej z klas.\n",
    "        val_multiclass_recalls = recall_score(val_labels,val_predictions,average=None,zero_division=0)\n",
    "        val_multiclass_precisions = precision_score(val_labels,val_predictions,average=None,zero_division=0)\n",
    "        val_multiclass_f1s = f1_score(val_labels,val_predictions,average=None,zero_division=0)\n",
    "        # Zapisujƒô te wyniki do zmiennych by zapisaƒá ca≈ÇƒÖ historiƒô uczenia.\n",
    "        self.val_recalls.append(val_multiclass_recalls)\n",
    "        self.val_precisions.append(val_multiclass_precisions)\n",
    "        self.val_f1s.append(val_multiclass_f1s)\n",
    "\n",
    "        self.avg_val_recall = round(mean(val_multiclass_recalls),2)\n",
    "        self.avg_val_precision = round(mean(val_multiclass_precisions),2)\n",
    "        self.avg_val_f1 = round(mean(val_multiclass_f1s),2)\n",
    "\n",
    "        self.avg_val_f1_history.append(self.avg_val_f1)\n",
    "\n",
    "        tf.print(f\"val_recall = {self.avg_val_recall} , val_prec = {self.avg_val_precision} , val_f1 = {self.avg_val_f1}\")\n",
    "                \n",
    "class EarlyStopByF1(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, patience = 0):\n",
    "        super(tf.keras.callbacks.Callback,self).__init__()\n",
    "        self.patience = patience\n",
    "    def on_epoch_end(self,epoch,logs={}):\n",
    "        f1_history = extended_history.avg_val_f1_history\n",
    "        f1 = extended_history.avg_val_f1\n",
    "        if(epoch >= self.patience):\n",
    "            if f1 < min(f1_history[-self.patience:]):\n",
    "                tf.print(f\"Early Stopping: Epoch={epoch},Patience={self.patience}\")\n",
    "                self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÜ Trenowanie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_history = ExtendedHistoryCb()\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    batch_size=train_batch_size, \n",
    "    epochs=100,\n",
    "    verbose=1,\n",
    "    callbacks=[\n",
    "        extended_history,\n",
    "        EarlyStopByF1(patience=3),\n",
    "        LearningRateReducerCb()\n",
    "        ], \n",
    "    validation_data=val_ds,\n",
    "    shuffle=True,\n",
    "    class_weight = weights\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"model1_w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1ff99d39a20>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.load_weights(\"model1_w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Wizualizacja Trenowania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "accuracy = history.history['accuracy']\n",
    "val_loss = history.history['val_loss']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title('Training And Validation Loss')\n",
    "plt.plot(loss,label=\"Traning Loss\")\n",
    "plt.plot(val_loss,label=\"Validation Loss\")\n",
    "plt.legend()\n",
    "plt.subplot(1,2,2)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title('Training And Validation Accuracy')\n",
    "plt.plot(accuracy,label=\"Training Accuracy\")\n",
    "plt.plot(val_accuracy,label=\"Validation Accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_recall = extended_history.val_recalls\n",
    "val_precision = extended_history.val_precisions\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Precision Per Category\")\n",
    "plt.title('Training And Validation Precisions')\n",
    "plt.plot(val_precision,label=\"Validation Precision\")\n",
    "plt.legend(range(n_categories))\n",
    "plt.subplot(1,2,2)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Recall Per Category\")\n",
    "plt.title('Training And Validation Recalls')\n",
    "plt.plot(val_recall,label=\"Validation Recall\")\n",
    "plt.legend(range(n_categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚äπ Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix1(ds,n_categories,model):\n",
    "    \"\"\"\n",
    "    Evaluates model's performance on a dataset. Prints confusion\n",
    "    and returns full report.\n",
    "    ds - Dataset object with data to predict labels for.\n",
    "    n_categories - number of possible label outcomes.\n",
    "    model - model used to predict labels.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10,8))\n",
    "    y_pred = tf.math.argmax(model.predict(ds),axis=1).numpy()\n",
    "    y_true = np.concatenate([y for x, y in ds],axis=0)\n",
    "    confusion = tf.math.confusion_matrix(y_true,y_pred,num_classes=n_categories)\n",
    "    report = classification_report(y_true,y_pred,zero_division=0)\n",
    "    sns.heatmap(confusion,annot=confusion)\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anger': 0,\n",
       " 'disgust': 1,\n",
       " 'fear': 2,\n",
       " 'happy': 3,\n",
       " 'neutral': 4,\n",
       " 'sad': 5,\n",
       " 'surprise': 6}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face_categories_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(plot_confusion_matrix1(train_ds,n_categories,model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(plot_confusion_matrix1(val_ds,n_categories,model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(plot_confusion_matrix1(test_ds,n_categories,model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Kamerka**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 files belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# tu umie≈õƒá obrazki\n",
    "# C:\\Users\\matlaczj\\Documents\\Projekt_PZ1\\dataset_splitted\\pred\\pred\n",
    "\n",
    "pred_ds = image_dataset_from_directory(\n",
    "    directory=pred_dir,\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=1,\n",
    "    image_size=(img_height, img_width),\n",
    "    shuffle=False,\n",
    "    seed=1,\n",
    "    interpolation='bilinear',\n",
    "    smart_resize = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot__next_image_from_pred_ds():\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    for images, _ in pred_ds.take(1):\n",
    "        plt.imshow(images[0].numpy().astype(\"uint8\"))\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset element_spec=(TensorSpec(shape=(None, 48, 48, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_ds.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anger', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = model.predict(pred_ds)\n",
    "predictions = dict(zip(face_categories,predictions.tolist()[3]))\n",
    "plt.bar(*zip(*predictions.items()));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d560dcdfd20c698553b721a5649b3492882c563ef16cf48416aff55369c53628"
  },
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
